import warnings
import numpy as np
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def generate_question(llm, retriever, previous_questions, verbose=False):
    """
    Generates a unique question based on the provided context.

    Args:
        llm (ChatOpenAI): An instance of the ChatOpenAI object.
        retriever (Retriever): An instance of the Retriever object.
        previous_questions (list): List of questions that have already been asked.
        verbose (bool, optional): Whether to print verbose output. Defaults to False.

    Returns:
        str: The generated question.
    """
    if verbose:
        print("Questions already asked: ", previous_questions)
        print("Generating question...")

    question_template = """
    You are an helpful assistant that acts as an examination bot. Your task is to generate unique questions that cover a wide range of topics and perspectives related to the provided context.
    The questions should be standalone and not reference the context. They should be clear and specific, not requiring access to the context to be understood.
    Under no circumstances should you repeat questions from the {previous_questions} list.
    Do not reference the document, the source of the context, or use phrases like "in the document", "discussed in the document", "in the context provided", "in the provided context" or similar phrases.
    For instance, don't say "What is the significance of U being an orthonormal matrix in the provided context?". Instead, say "What is the significance of U being an orthonormal matrix?".
    <context>
    {context}
    </context>
    Question: <question>        
    """
    question_prompt = ChatPromptTemplate.from_template(question_template)
    rag_question_chain = (
        {"context": retriever, "previous_questions": RunnablePassthrough()}
        | question_prompt
        | llm
        | StrOutputParser()
    )
    response = rag_question_chain.invoke(str(previous_questions))
    if verbose:
        print("Generated question: ", response)
        print("Text passages used for question generation: ",
              retriever.get_relevant_documents(response))
    return response

def generate_answer(llm, retriever, question, verbose=False):
    """
    Generates an answer based on the provided context and question.

    Args:
        llm (ChatOpenAI): An instance of the ChatOpenAI object.
        retriever (Retriever): An instance of the Retriever object.
        question (str): The question to answer.
        verbose (bool, optional): Whether to print verbose output. Defaults to False.

    Returns:
        str: The generated answer.
    """
    if verbose:
        print("Generating answer...")

    answer_template = """
    Answer the following question based only on the provided context. Do not use phrases like "in the provided context" or similar patterns in the answer.
    For instance, don't say "In the provided context, the significance of U being an orthonormal matrix is...". Instead, say "The significance of U being an orthonormal matrix is...".
    <context>
    {context}
    </context>
    Question: {question}
    """
    answer_prompt = ChatPromptTemplate.from_template(answer_template)
    answer_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | answer_prompt
        | llm
        | StrOutputParser()
    )
    response = answer_chain.invoke(question)
    if verbose:
        print("Generated answer: ", response)
        print("Text passages used for answer generation: ",
              retriever.get_relevant_documents(response))
    return response

def evaluate_qa(llm, question, correct_answer, user_answer, verbose=False):
    """
    DEPRECATED: This function is deprecated and has been replaced by evaluate_qa_with_cosine_similarity.

    Evaluates the user's answer to a question against the AI's answer.

    Args:
        llm (ChatOpenAI): An instance of the ChatOpenAI object.
        question (str): The question asked.
        correct_answer (str): The correct answer generated by the AI model.
        user_answer (str): The user's answer to the question.
        verbose (bool, optional): Whether to print verbose output. Defaults to False.

    Returns:
        str: The evaluation of the user's answer.
    """
    warnings.warn("evaluate_qa function is deprecated and might be removed in future versions. Use evaluate_qa_with_cosine_similarity instead.", DeprecationWarning)
    
    if verbose:
        print("Evaluating...")

    evaluation_template = """
    Consider the following question: {question}
    The correct answer to this question is: {correct_answer}
    The user's answer to this question is: {user_answer}
    Evaluate the user's answer. If it matches the correct answer or is semantically similar, acknowledge it and respond with "That's corret!". If it doesn't, respond with "That's not quite right. The correct answer is: " and then provide the correct answer to help them understand.
    """
    evaluation_prompt = ChatPromptTemplate.from_template(evaluation_template)
    evaluation_chain = (
        evaluation_prompt 
        | llm 
        | StrOutputParser()
    )

    response = evaluation_chain.invoke({"question": question, "correct_answer": correct_answer, "user_answer": user_answer})
    if verbose:
        print("Evaluation: ", response)
    return response

def grading_system(cosine_similarity_score, correct_answer):
    """
    Grades the user's answer based on the cosine similarity score.

    Args:
        cosine_similarity_score (float): The cosine similarity score between the correct answer and the user's answer.
        correct_answer (str): The correct answer to the question.

    Returns:
        str: The feedback based on the cosine similarity score.
    """
    if cosine_similarity_score >= 0.90:
        feedback = "10/10 - Excellent answer!"
    elif cosine_similarity_score >= 0.70:
        feedback = "7/10 - Good answer, but there's room for improvement."
    elif cosine_similarity_score >= 0.50:
        feedback = "5/10 - Decent answer, but some important details were missed."
    elif cosine_similarity_score >= 0.30:
        feedback = "3/10 - Poor answer, you need to work on your understanding."
    else:
        feedback = "0/10 - Incorrect answer, you need to review the material."
    
    if cosine_similarity_score < 0.90:
        feedback += f" The correct answer is: {correct_answer}"
    return feedback


def cosine_similarity(correct_answer_vector, user_answer_vector):
    """
    Calculates the cosine similarity between two vectors.

    Args:
        correct_answer_vector (list): The vector representation of the correct answer.
        user_answer_vector (list): The vector representation of the user's answer.

    Returns:
        float: The cosine similarity between the two vectors.
    """
    return round(np.dot(correct_answer_vector, user_answer_vector) / (np.linalg.norm(correct_answer_vector) * np.linalg.norm(user_answer_vector)), 2)

def evaluate_qa_with_cosine_similarity(embedding_model, correct_answer, user_answer, verbose=False):
    """
    Evaluates the user's answer to a question against the correct answer using cosine similarity.

    Args:
        embedding_model (OpenAIEmbeddings): An instance of the OpenAIEmbeddings object.
        correct_answer (str): The correct answer generated by the AI model.
        user_answer (str): The user's answer to the question.
        verbose (bool, optional): Whether to print verbose output. Defaults to False.

    Returns:
        str: The feedback based on the cosine similarity score.
    """
    if verbose:
        print("Evaluating...")
    
    correct_answer_embedding = embedding_model.embed_query(correct_answer)
    user_answer_embedding = embedding_model.embed_query(user_answer)
    cos_similarity = cosine_similarity(correct_answer_embedding, user_answer_embedding)
    feedback = grading_system(cos_similarity, correct_answer)
    if verbose:
        print("Cosine similarity score: ", cos_similarity)
    return feedback